\documentclass{article} 
\usepackage{geometry}
\geometry{left=3.5cm, right=3.5cm, top=2.5cm, bottom=2.5cm}
\usepackage{graphicx} 
\usepackage{listings}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\usepackage{amsmath,amsthm}
\usepackage{amsmath}
\usepackage{float}
\usepackage{mhchem}
\usepackage{exercise}
\usepackage{subfigure}

\begin{document}  
\title{Homework 2}
\author{Liting Hu}

\maketitle

\section*{Problem 1}
\subsection*{1. Logistic regression}
\begin{verbatim}
# Logistic Regression
glm.fit <- glm(default~balance+student+income, data=Default, family=binomial)
summary(glm.fit)
summary(glm.fit$coefficients)

glm.probs <- predict(glm.fit, data=Default, type ="response")

glm.pred=rep("No", 10000)
glm.pred[glm.probs >.5]="Yes"

table(glm.pred, default)
mean(glm.pred == default)
# 0.9732

glm.probs <- predict(glm.fit, DefaultPredict, type="response")
glm.pred <- rep("No", dimDP[1])
glm.pred[glm.probs >.5] <- "Yes"
DefaultPredict$glm.pred <- glm.pred
\end{verbatim}

\subsection*{2. Linear discriminant analysis}
\begin{verbatim}
lda.fit <- lda(default~balance+student+income, data=Default)
lda.fit
plot(lda.fit)

lda.pred <- predict(lda.fit, Default)
lda.class <- lda.pred$class
table(lda.class, default)
mean(lda.class == default)
# 0.9724

sum(lda.pred$posterior[, 1] >= .5)
sum(lda.pred$posterior[, 1] < .5)
sum(lda.pred$posterior[, 1] > .9)

lda.probs <- predict(lda.fit, DefaultPredict, type="response")
lda.pred <- rep("No", dimDP[1])
lda.pred[lda.probs$posterior[, 2] > .5] <- "Yes"
DefaultPredict$lda.pred <- lda.pred
\end{verbatim}

\subsection*{3. quadratic discriminant analysis}
\begin{verbatim}
qda.fit <- qda(default~balance+student+income, data=Default)
qda.fit

qda.probs <- predict(qda.fit, Default)
qda.class <- qda.pred$class
table(qda.class, default)
mean(qda.class == default)
# 0.973

qda.probs <- predict(qda.fit, DefaultPredict, type="response")
DefaultPredict$qda.pred <- qda.probs$class
\end{verbatim}

\subsection*{4. K-nearest neighbor classification}
\begin{verbatim}
# K-Nearest Neighbors
Default.X <- cbind(Default$student, Default$balance, Default$income)
DefaultPredict.X <- cbind(DefaultPredict$student, DefaultPredict$balance, DefaultPredict$income)
knn.pred1 <- knn(Default.X, DefaultPredict.X, default, k = 1)
knn.pred3 <- knn(Default.X, DefaultPredict.X, default, k = 3)
knn.pred6 <- knn(Default.X, DefaultPredict.X, default, k = 6)
knn.pred10 <- knn(Default.X, DefaultPredict.X, default, k = 10)
DefaultPredict$knn.pred1 <- knn.pred1
DefaultPredict$knn.pred3 <- knn.pred3
DefaultPredict$knn.pred6 <- knn.pred6
DefaultPredict$knn.pred10 <- knn.pred10
detach(Default)
\end{verbatim}
\newpage
All results are stored in dataframe DefaultPredict:
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=16cm]{dp.png}  
\caption{Default predict under different measures} 
\end{center} 
\end{figure}
In K-nearest neighbor classification, the results under $k=1$ and $k=10$ seem unreasonable compared to other classifications. Choosing a $k$ value between 3 and 6 may produce a better result.

\section*{Problem 2}
\subsection*{1. Read the Email Messages}
\begin{verbatim}
spamPath = "/Users/apple/Desktop/582/RSpamData"
dirNames = list.files(path = paste(spamPath, "messages", 
                                   sep = .Platform$file.sep))
length(list.files(paste(spamPath, "messages", dirNames, 
                        sep = .Platform$file.sep)))
# [1] 9353
sapply(paste(spamPath, "messages", dirNames, 
             sep = .Platform$file.sep), 
       function(dir) length(list.files(dir)) )

fullDirNames = paste(spamPath, "messages", dirNames, 
                     sep = .Platform$file.sep)
\end{verbatim}

There are 9353 messages in the 5 directories combined.

\subsection*{2. Find the Words in a Message}
Firstly, define a function to split the message into header and body. The body of a message is separated from the header by a single empty line. 
\begin{verbatim}
splitMessage = function(msg) {
    splitPoint = match("", msg)
    header = msg[1:(splitPoint-1)]
    body = msg[ -(1:splitPoint) ]
    return(list(header = header, body = body))
}
\end{verbatim}

Then we should remove attachments from the message. Based on the anatomy of email messages, If an attachment is added to a message, the MIME type is multipart and the Content-Type field provides a boundary string that can be used to locate the attachments. To get the boundary, define function:

\begin{verbatim}
getBoundary = function(header) {
    boundaryIdx = grep("boundary=", header)
    boundary = gsub('"', "", header[boundaryIdx])
    gsub(".*boundary= *([^;]*);?.*", "\\1", boundary)
}
\end{verbatim}
Then we can remove attachments from messages by
\begin{verbatim}
dropAttach = function(body, boundary){
    
    bString = paste("--", boundary, sep = "")
    bStringLocs = which(bString == body)
    # Search for the boundary string
    if (length(bStringLocs) <= 1) return(body)
    
    eString = paste("--", boundary, "--", sep = "")
    eStringLoc = which(eString == body)
    # The closing boundary
    if (length(eStringLoc) == 0) 
        return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
    
    n = length(body)
    if (eStringLoc < n) 
        return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                         ( (eStringLoc + 1) : n )) ] )
    
    return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}
\end{verbatim}

After all these steps, words can be extracted from the messages.

\begin{verbatim}
cleanText =
    function(msg)   {
        tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
    }

findMsgWords = 
    function(msg) {
        if(is.null(msg))
            return(character())
        
        words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
        
        # drop empty and 1 letter words
        words = words[ nchar(words) > 1]
        invisible(words)
    }

processAllWords = function(dirName)
{
    # read all files in the directory
    fileNames = list.files(dirName, full.names = TRUE)
    # drop files that are not email, i.e., cmds
    notEmail = grep("cmds$", fileNames)
    if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]
    
    messages = lapply(fileNames, readLines, encoding = "latin1")
    
    # split header and body
    emailSplit = lapply(messages, splitMessage)
    # put body and header in own lists
    bodyList = lapply(emailSplit, function(msg) msg$body)
    headerList = lapply(emailSplit, function(msg) msg$header)
    rm(emailSplit)
    
    # determine which messages have attachments
    hasAttach = sapply(headerList, function(header) {
        CTloc = grep("Content-Type", header)
        if (length(CTloc) == 0) return(0)
        multi = grep("multi", tolower(header[CTloc])) 
        if (length(multi) == 0) return(0)
        multi
    })
    
    hasAttach = which(hasAttach > 0)
    
    # find boundary strings for messages with attachments
    boundaries = sapply(headerList[hasAttach], getBoundary)
    
    # drop attachments from message body
    bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                                 boundaries, SIMPLIFY = FALSE)
    
    # extract words from body
    msgWordsList = lapply(bodyList, findMsgWords)
    
    invisible(msgWordsList)
}
msgWordsList = lapply(fullDirNames, processAllWords)  
\end{verbatim}

For now we have collected all the necessary words from all the emails. Create a logical vector to define whether the messages are spam or not based on the number of elements in each list and flatten all five lists into one list:

\begin{verbatim}
numMsgs = sapply(msgWordsList, length)
numMsgs
# [1] 5051 1400  500 1000 1397

isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)
msgWordsList = unlist(msgWordsList, recursive = FALSE)
\end{verbatim}

\subsection*{3. The Naive Bayes Classifier}

For spam messages (similar to ham messages), we should estimate the following probabilities (1/2 used to avoid zero probabilities):
\[P({\rm{a word is present|spam}}) \approx \frac{{\# {\rm{of spam messages with this word}} + 1/2}}{{\# {\rm{of spam messages}} + 1/2}}\]
\[P({\rm{a word is absent|spam}}) \approx \frac{{\# {\rm{of spam messages without this word}} + 1/2}}{{\# {\rm{of spam messages}} + 1/2}}\]

We use log of ratios to avoid products and it tends to have better statistical properties. So for a new message we compute the log likelihood ratio as

\[\begin{array}{l}\sum\limits_{{\rm{words in message}}} {(\log } P({\rm{word present|spam}}) - \log P({\rm{word present|ham}})) + \\\sum\limits_{{\rm{words not in message}}} {(\log P({\rm{word absent|spam}}) - \log P({\rm{word absent|ham}})) + \log P({\rm{spam}}) - \log P({\rm{ham}})} 
\end{array}\]


where ${\log P({\rm{spam}}) - \log P({\rm{ham}})}$ should be constant. \\
To collect these probabilities, construct the function that returns all log likelihood ratios as a matrix:

\begin{verbatim}
computeFreqs =
    function(wordsList, spam, bow = unique(unlist(wordsList)))
    {
        # create a matrix for spam, ham, and log odds
        wordTable = matrix(0.5, nrow = 4, ncol = length(bow), 
                           dimnames = list(c("spam", "ham", 
                                             "presentLogOdds", 
                                             "absentLogOdds"),  bow))
        
        # For each spam message, add 1 to counts for words in message
        counts.spam = table(unlist(lapply(wordsList[spam], unique)))
        wordTable["spam", names(counts.spam)] = counts.spam + .5
        
        # Similarly for ham messages
        counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  
        wordTable["ham", names(counts.ham)] = counts.ham + .5  
        
        
        # Find the total number of spam and ham
        numSpam = sum(spam)
        numHam = length(spam) - numSpam
        
        # Prob(word|spam) and Prob(word | ham)
        wordTable["spam", ] = wordTable["spam", ]/(numSpam + .5)
        wordTable["ham", ] = wordTable["ham", ]/(numHam + .5)
        
        # log odds
        wordTable["presentLogOdds", ] = 
            log(wordTable["spam",]) - log(wordTable["ham", ])
        wordTable["absentLogOdds", ] = 
            log((1 - wordTable["spam", ])) - log((1 -wordTable["ham", ]))
        
        invisible(wordTable)
    }

trainTable = computeFreqs(trainMsgWords, trainIsSpam)
\end{verbatim}

The trainTable can be used to compute the log likelihood ratio for a new message. This value is used to classify the message as spam or ham. Besides, we apply function computeMsgLLR to each of the messages in our test set

\begin{verbatim}
computeMsgLLR = function(words, freqTable) 
{
    # Discards words not in training data.
    words = words[!is.na(match(words, colnames(freqTable)))]
    
    # Find which words are present
    present = colnames(freqTable) %in% words
    
    sum(freqTable["presentLogOdds", present]) +
        sum(freqTable["absentLogOdds", !present])
}
\end{verbatim}
 To compare the summary statistics of the LLR values for the ham and spam in the test data with
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{sta.png}  
\label{sta} 
\end{center} 
\end{figure}
and the boxplots for these data: 
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{boxplot.png}  
\caption{Boxplot of log likelyhood ratio for spam and ham} 
\label{spd} 
\end{center} 
\end{figure}
which shows that most ham data have log likelihood ratio below zero and spam data above zero. So naive Bayes approximation is a good way to group spam or ham messages. And we need to decide a cut-off $\tau$ which used to judge a message is spam or not. \\

Before that, define functions to calculate type I and type II errors. Then draw the plot to display these two errors in different $\tau$.
\begin{verbatim}
typeIErrorRates = 
    function(llrVals, isSpam) 
    {
        o = order(llrVals)
        llrVals =  llrVals[o]
        isSpam = isSpam[o]
        
        idx = which(!isSpam)
        N = length(idx)
        list(error = (N:1)/N, values = llrVals[idx])
    }

typeIIErrorRates = function(llrVals, isSpam) {
    
    o = order(llrVals)
    llrVals =  llrVals[o]
    isSpam = isSpam[o]
    
    idx = which(isSpam)
    N = length(idx)
    list(error = (1:(N))/N, values = llrVals[idx])
}  
\end{verbatim}

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{typeerrors.png}  
\caption{Type I and II Error Rates} 
\label{spd} 
\end{center} 
\end{figure}
with a threshold of $\tau =-40$, messages with an LLR value above -40 will be classified as spam messages while others as ham. Since in this case, type I error is 0.01 and type II error is 0.023, 1\% of ham is misclassified as spam and 2.3\% of spam is misclassified as ham.

\subsection*{4. Result by removing the stop words}
Stop words also can be found in ``tm'' package and they are easily extracted as a vector.
\begin{verbatim}
library(tm)
stopwords <- stopwords()
\end{verbatim}
Then add a function after processAllWords to remove stop words as:
\begin{verbatim}
removeStopWords =
    function(x, stopWords)
    {
        stopWords <- stopwords()
        if(is.character(x))
            setdiff(x, stopWords)
        else if(is.list(x))
            lapply(x, removeStopWords, stopWords)
        else x
    }

msgWordsList = lapply(msgWordsList, removeStopWords, stopWords)  
\end{verbatim}

Other steps are similar. Then we have the result under the condition that the stop words are removed. The summary statistics of the LLR values for the ham and spam in the test data are
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{sta2.png}  
\label{spd} 
\end{center} 
\end{figure}
And the boxplot: 
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{boxplot2.png}  
\caption{Boxplot of log likelyhood ratio for spam and ham when stop words are removed} 
\label{spd} 
\end{center} 
\end{figure}
Type I and type II Error Rates:
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{typeerrors.png}  
\caption{Type I and II Error Rates when stop words are removed} 
\label{spd} 
\end{center} 
\end{figure}
That is just a little different with the situation that remains stop words. Setting $\tau =-43$, messages with an LLR value above -43 will be classified as spam messages while others as ham. Since in this case, type I error is 0.01 and type II error is 0.02, 1\% of ham is misclassified as spam and 2\% of spam is misclassified as ham.


\section*{Problem 3}
\subsection*{1. Get and Read the Data}
Firstly, download the historical stock data from Yahoo Finance and save these contents to local files. 
\begin{verbatim}
library(XML)
u = paste("http://real-chart.finance.yahoo.com/table.csv",
                "s=PEP&a=5&b=1&c=1972&d=10&e=9&f=2016&g=d",sep = "?")
download.file(u,"PEP.csv")
u = paste("http://real-chart.finance.yahoo.com/table.csv",
                "s=KO&a=0&b=2&c=1962&d=10&e=9&f=2016&g=d",sep = "?")
download.file(u,"KO.csv")
u = paste("http://real-chart.finance.yahoo.com/table.csv",
                "s=DPS&a=4&b=7&c=2008&d=10&e=9&f=2016&g=d",sep = "?")
download.file(u,"DPS.csv")
# Download CSV files
\end{verbatim}
Then read the data and make sure that the date values remain as strings and not interpreted as a factor but converted into date format.
\begin{verbatim}
readData =
    #read the data and convert the Date column to an object of class Date.  
    function(fileName, dateFormat = c("%Y-%m-%d", "%Y/%m/%d"), ...)
    {
        data = read.csv(fileName, header = TRUE, 
                        stringsAsFactors = FALSE, ...)
        for(fmt in dateFormat) {
            tmp = as.Date(data$Date, fmt)
            if(all(!is.na(tmp))) {
                data$Date = tmp
                break
            }
        }
        
        data[ order(data$Date), ]
    }

pep <- readData("PEP.csv")
ko <- readData("KO.csv")
dps <- readData("DPS.csv")
\end{verbatim}

\subsection*{2. Proceed the Data}

For now we use stock ``PEP'' and ``KO'' to do the pairs trading. \\

Define a function to compute the subsets with common dates and return a data frame with records for each day with the adjusted closing prices for the two stocks. After combining stock ``PEP'' and ``KO'', we can create the ratio of the adjusted closing price.

\begin{verbatim}
combine2Stocks = 
    function(a, b, stockNames = c(deparse(substitute(a)), 
                                  deparse(substitute(b))))
    {
        rr = intersect(a$Date, b$Date)
        a.sub=a[which(a$Date %in% rr),]
        b.sub=b[which(b$Date %in% rr),]
        structure(data.frame(as.Date(a.sub$Date), 
                             a.sub$Adj.Close, 
                             b.sub$Adj.Close),
                  names = c("Date", stockNames))
    }
overlap = combine2Stocks(pep, ko)
r = overlap$pep/overlap$ko
\end{verbatim}

\noindent Visualize the ratio ($k=1$):

\begin{verbatim}
plotRatio =
    function(r, k = 1, date = seq(along = r), ...)
    {
        plot(date, r, type = "l", ...)
        abline(h = c(mean(r), 
                     mean(r) + k * sd(r), 
                     mean(r) - k * sd(r)), 
               col = c("darkgreen", rep("red", 2*length(k))), 
               lty = "dashed")
    }

plotRatio(r, k = 1, overlap$Date, col = "lightgray", xlab = "Date", ylab = "Ratio")
length(r)
\end{verbatim}

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{ratio.png}  
\caption{The ratio of stock prices for PEP and KO} 
\label{ratio} 
\end{center} 
\end{figure}

\subsection*{3. Finding positions}
In our case, $k=1$, the upper bound is $m+ks$ and the lower bound is $m-ks$. Compute two opening positions (green dots) and two closing positions (red dots). The plot was drawn below.

\begin{verbatim}
findNextPosition =
    # Check they are increasing and correctly offset
    function(ratio, startDay = 1, k = 1, 
             m = mean(ratio), s = sd(ratio))
    {
        up = m + k *s
        down = m - k *s
        
        if(startDay > 1)
            ratio = ratio[ - (1:(startDay-1)) ]
        
        isExtreme = ratio >= up | ratio <= down
        
        if(!any(isExtreme))
            return(integer())
        
        start = which(isExtreme)[1]
        backToNormal = if(ratio[start] > up)
            ratio[ - (1:start) ] <= m
        else
            ratio[ - (1:start) ] >= m
        
        # return either the end of the position or the index 
        # of the end of the vector.
        # Could return NA for not ended, i.e. which(backToNormal)[1]
        # for both cases. But then the caller has to interpret that.
        
        end = if(any(backToNormal))
            which(backToNormal)[1] + start
        else
            length(ratio)
        
        c(start, end) + startDay - 1 
    }

k = 1
a = findNextPosition(r, k = k)         #1 1095
b = findNextPosition(r, a[2], k = k)   #2186 2661

symbols(overlap$Date[a[1]],r[a[1]],circles = 60,fg = "darkgreen", add=TRUE, inches = FALSE)
symbols(overlap$Date[a[2]],r[a[2]],circles = 60,fg = "red", add=TRUE, inches = FALSE)
symbols(overlap$Date[b[1]],r[b[1]],circles = 60,fg = "darkgreen", add=TRUE, inches = FALSE)
symbols(overlap$Date[b[2]],r[b[2]],circles = 60,fg = "red", add=TRUE, inches = FALSE)
\end{verbatim}

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{tests.png}  
\caption{Position finding function test} 
\label{testp} 
\end{center} 
\end{figure}
In order to find all positions, define the function ``getPositions'':
\begin{verbatim}
getPositions =
    function(ratio, k = 1, m = mean(ratio), s = sd(ratio))
    {
        when = list()
        cur = 1
        
        while(cur < length(ratio)) {
            tmp = findNextPosition(ratio, cur, k, m, s)
            if(length(tmp) == 0)  # done
                break
            when[[length(when) + 1]] = tmp
            if(is.na(tmp[2]) || tmp[2] == length(ratio))
                break
            cur = tmp[2]
        }
        
        when
    }

pos <- getPositions(r,k)
plotRatio(r, k, overlap$Date, col = "lightgray", xlab = "Date", ylab = "Ratio")
invisible(lapply(pos, function(p) showPosition(overlap$Date[p],r[p])))
\end{verbatim}
Then draw the plot. 
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{allp.png}  
\caption{All positions when k=1} 
\label{testp1} 
\end{center} 
\end{figure}

Set $k=0.5$, the plot is showed below. There are much more positions than that in $k=1$.

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=10cm]{allp5.png}  
\caption{All positions when k=0.5} 
\label{testp} 
\end{center} 
\end{figure}

More positions existing means that we may earn more but cost more in transaction fees. To pursue the maximum earnings, we should find appropriate value k based on the historical data for our pair of stocks.

\subsection*{4. Computing the Profit and Find the Optimal Value for k}
Calculate the position profits: 
\begin{verbatim}
positionProfit =
    #  r = overlap$pep/overlap$ko
    #  pos = getPositions(r, k)
    #  positionProfit(pos[[1]], overlap$pep, overlap$ko)
    function(pos, stockPriceA, stockPriceB, 
             ratioMean = mean(stockPriceA/stockPriceB), 
             p = .0002, byStock = FALSE)
    {
        if(is.list(pos)) {
            ans = sapply(pos, positionProfit, 
                         stockPriceA, stockPriceB, ratioMean, p, byStock)
            if(byStock)
                rownames(ans) = c("A", "B", "commission")
            return(ans)
        }
        # prices at the start and end of the positions
        priceA = stockPriceA[pos]
        priceB = stockPriceB[pos]
        
        # how many units can we by of A and B with $1
        unitsOfA = 1/priceA[1]
        unitsOfB = 1/priceB[1]
        
        # The dollar amount of how many units we would buy of A and B
        # at the cost at the end of the position of each.
        amt = c(unitsOfA * priceA[2], unitsOfB * priceB[2])
        
        # Which stock are we selling
        sellWhat = if(priceA[1]/priceB[1] > ratioMean) "A" else "B"
        
        profit = if(sellWhat == "A") 
            c((1 - amt[1]),  (amt[2] - 1), - p * sum(amt))
        else 
            c( (1 - amt[2]),  (amt[1] - 1),  - p * sum(amt))
        
        if(byStock)
            profit
        else
            sum(profit)
    }
\end{verbatim}
When calculating for PEP/KO ratios, the result is: 
\begin{verbatim}
[1]  0.37333954  0.42307159  0.99590876  0.34030945  0.84320973  0.35930336 -0.04535196
\end{verbatim}
which seems reasonable.\\
Then to find the optimal value for k, divide the data into a training period and a test period. we just use the training period to determine optimal value. 

\begin{verbatim}
i = 1:floor(nrow(overlap)/2)
train = overlap[i, ]
test = overlap[ - i, ]
# Divide the data half to a training period and and half to a test period

r.train = train$pep/train$ko
r.test = test$pep/test$ko

k.max = max((r.train - mean(r.train))/sd(r.train))
k.min = min((abs(r.train - mean(r.train))/sd(r.train)))
ks = seq(k.min, k.max, length = 1000)
m  = mean(r.train)

profits =
    sapply(ks,
           function(k) {
               pos = getPositions(r.train, k)
               sum(positionProfit(pos, train$pep, train$ko, 
                                  mean(r.train)))
           })

plot(ks, profits, type = "l", xlab = "k", ylab = "Profit")
tmp.k = ks[profits == max(profits)]  
# [1] 0.03661572
pos = getPositions(r.test, tmp.k, mean(r.train), sd(r.train))
testProfit = sum(positionProfit(pos, test$pep, test$ko)) 
testProfit
# [1] 1.548414
\end{verbatim}
Loop over 1000 values of k and compute the 1000 corresponding profits. We can see the relationship between k and profits as below

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=12cm]{maxk.png}  
\caption{Relationship between profits and k} 
\end{center} 
\end{figure}
The profit reach its maximum 1.548414 (about 155\%) at $k=0.03661572$.

\subsection*{5. Pairs trading on PEP and DPS}
All steps are similar to former trading.
\begin{verbatim}
# PEP vs DPS
overlap_pd <- combine2Stocks(pep, dps)
r_pd <- overlap_pd$ko/overlap_pd$dps

i = 1:floor(nrow(overlap_pd)/2)
train = overlap_pd[i, ]
test = overlap_pd[ - i, ]
# Divide the data half to a training period and and half to a test period

r.train = train$pep/train$dps
r.test = test$pep/test$dps
sd(r.train)
k.max = min(max((r.train - mean(r.train))/sd(r.train)), 
            (mean(r.train)-min(r.train))/sd(r.train))
#k.max = max((r.train - mean(r.train))/sd(r.train))
k.min = max(0.1, min((abs(r.train - mean(r.train))/sd(r.train))))
ks = seq(k.min, k.max, length = 1000)
m  = mean(r.train)

profits =
    sapply(ks,
           function(k) {
               pos = getPositions(r.train, k)
               sum(positionProfit(pos, train$pep, train$dps, 
                                  mean(r.train)))
           })

plot(ks, profits, type = "l", xlab = "k", ylab = "Profit")
tmp.k = ks[profits == max(profits)]  
#[1] 1.061245 1.062219 1.063193 1.064167 1.065141 1.066115 1.067089 1.068063 1.069037 1.070011
#[11] 1.070984 1.071958

k.star = mean(ks[profits == max(profits)])
# [1] 1.066602

pos = getPositions(r.test, k.star)
testProfit = sum(positionProfit(pos, test$pep, test$dps)) 
testProfit
\end{verbatim}

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=12cm]{kpd.png}  
\caption{Relationship between profits and k in pairs trading PEP vs DPS} 
\end{center} 
\end{figure}
The profit reach its maximum 0.3586618 (about 35.9\%) at $k=1.066602$.
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=12cm]{pos1.png}  
\end{center} 
\end{figure}

\subsection*{6. Pairs trading on KO and DPS}
\begin{verbatim}
# KO vs DPS
overlap_kd <- combine2Stocks(ko, dps)
r_kd <- overlap_kd$ko/overlap_kd$dps

i = 1:floor(nrow(overlap_kd)/2)
train = overlap_kd[i, ]
test = overlap_kd[ - i, ]
# Divide the data half to a training period and and half to a test period

r.train = train$ko/train$dps
r.test = test$ko/test$dps

k.max = max((r.train - mean(r.train))/sd(r.train))
k.min = max(0.1, min((abs(r.train - mean(r.train))/sd(r.train))))
ks = seq(k.min, k.max, length = 1000)
m  = mean(r.train)

profits =
    sapply(ks,
           function(k) {
               pos = getPositions(r.train, k)
               sum(positionProfit(pos, train$ko, train$dps, 
                                  mean(r.train)))
           })

plot(ks, profits, type = "l", xlab = "k", ylab = "Profit")
tmp.k = ks[profits == max(profits)]  
# [1] 0.4365644 0.4402227

k.star = mean(ks[profits == max(profits)])
# [1] 0.4383935

pos = getPositions(r.test, k.star)
testProfit = sum(positionProfit(pos, test$ko, test$dps)) 
testProfit
\end{verbatim}

\begin{figure}[H] 
\begin{center} 
\includegraphics[width=12cm]{kkd.png}  
\caption{Relationship between profits and k in pairs trading KO vs DPS} 
\end{center} 
\end{figure}
The profit reach its maximum 0.1493933 (about 14.9\%) at $k=0.4383935$.
\begin{figure}[H] 
\begin{center} 
\includegraphics[width=12cm]{pos2.png}  
\end{center} 
\end{figure}


\end{document}